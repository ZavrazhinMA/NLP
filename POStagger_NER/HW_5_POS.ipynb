{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "**Написать теггер на данных с руским языком**\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:46:52.408365Z",
     "start_time": "2021-08-15T13:46:51.908346Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:46:53.900503Z",
     "start_time": "2021-08-15T13:46:52.410347Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyconll\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:46:53.916484Z",
     "start_time": "2021-08-15T13:46:53.902483Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "# !wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:47:15.577503Z",
     "start_time": "2021-08-15T13:46:53.919485Z"
    }
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:47:16.119502Z",
     "start_time": "2021-08-15T13:47:15.578495Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "test_sent = []\n",
    "\n",
    "for sent in full_train[:]:\n",
    "    train_data.append([(token.form, token.upos) for token in sent])\n",
    "\n",
    "for sent in full_test[:]:\n",
    "    test_data.append([(token.form, token.upos) for token in sent])\n",
    "    test_sent.append([(token.form, token.upos) for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:47:16.135480Z",
     "start_time": "2021-08-15T13:47:16.120495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Таким', 'DET'),\n",
       "  ('образом', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('некоторые', 'DET'),\n",
       "  ('инструкции', 'NOUN'),\n",
       "  ('должны', 'ADJ'),\n",
       "  ('выполняться', 'VERB'),\n",
       "  ('строго', 'ADV'),\n",
       "  ('после', 'ADP'),\n",
       "  ('завершения', 'NOUN'),\n",
       "  ('работы', 'NOUN'),\n",
       "  ('инструкций', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('от', 'ADP'),\n",
       "  ('которых', 'PRON'),\n",
       "  ('они', 'PRON'),\n",
       "  ('зависят', 'VERB'),\n",
       "  ('.', 'PUNCT')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:47:16.151482Z",
     "start_time": "2021-08-15T13:47:16.136480Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_tagger(train_data, test_data, tagger_cl, backoff=None):\n",
    "    tagger = tagger_cl(train_data, backoff=backoff)\n",
    "    accuracy = tagger.evaluate(test_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:47:30.161481Z",
     "start_time": "2021-08-15T13:47:16.154488Z"
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "tagger_list = [UnigramTagger, BigramTagger, TrigramTagger]\n",
    "\n",
    "for tagger_cl in tagger_list:\n",
    "    results[tagger_cl.__name__] = {\n",
    "        'accuracy': test_tagger(train_data, test_data, tagger_cl)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:48:28.699502Z",
     "start_time": "2021-08-15T13:47:30.163484Z"
    }
   },
   "outputs": [],
   "source": [
    "for tagger_cl in tagger_list:\n",
    "    for backoff in tagger_list:\n",
    "        if tagger_cl.__name__ == backoff.__name__:\n",
    "            continue\n",
    "        back_name = f'{tagger_cl.__name__}/{backoff.__name__}'\n",
    "        backoff_cl = backoff(train_data)\n",
    "        results[back_name] = {'accuracy': test_tagger(\n",
    "            train_data, test_data, tagger_cl, backoff=backoff_cl)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:48:43.720502Z",
     "start_time": "2021-08-15T13:48:28.700495Z"
    }
   },
   "outputs": [],
   "source": [
    "def backoff_tagger(train_data, tagger_cl, backoff=None):\n",
    "    for cls in tagger_cl:\n",
    "        backoff = cls(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = UnigramTagger(train_data)\n",
    "tag = backoff_tagger(train_data,\n",
    "                     [BigramTagger, TrigramTagger],\n",
    "                     backoff=backoff)\n",
    "results['TrigramTagger/BigramTagger/UnigramTagger'] = {\n",
    "    'accuracy': tag.evaluate(test_data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. написать свой теггер, попробовать разные векторайзеры, добавить знание не только букв но и слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:48:43.736502Z",
     "start_time": "2021-08-15T13:48:43.722483Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:48:44.286502Z",
     "start_time": "2021-08-15T13:48:43.738484Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in train_data[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "\n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in test_data[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:48:44.892503Z",
     "start_time": "2021-08-15T13:48:44.287484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T13:48:44.908502Z",
     "start_time": "2021-08-15T13:48:44.893495Z"
    }
   },
   "outputs": [],
   "source": [
    "hvectorizer1 = HashingVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "hvectorizer2 = HashingVectorizer(n_features=100)\n",
    "\n",
    "cvectorizer1 = CountVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "cvectorizer2 = CountVectorizer()\n",
    "\n",
    "tvectorizer1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "tvectorizer2 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T14:09:13.842040Z",
     "start_time": "2021-08-15T13:48:44.910484Z"
    }
   },
   "outputs": [],
   "source": [
    "vect_list = [hvectorizer1, hvectorizer2, cvectorizer1,\n",
    "             cvectorizer2, tvectorizer1, tvectorizer2]\n",
    "\n",
    "for num, vectorizer in enumerate(vect_list, 1):\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)\n",
    "    model = LogisticRegression(random_state=13)\n",
    "    model.fit(X_train, train_enc_labels)\n",
    "    pred = model.predict(X_test)\n",
    "    analyzer = 'word' if num % 2 == 0 else 'char'\n",
    "    results[type(vectorizer).__name__ + '_' +\n",
    "           analyzer] = {'accuracy': accuracy_score(test_enc_labels, pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-15T14:09:13.874042Z",
     "start_time": "2021-08-15T14:09:13.844040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CountVectorizer_char</th>\n",
       "      <td>0.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfidfVectorizer_char</th>\n",
       "      <td>0.9382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HashingVectorizer_char</th>\n",
       "      <td>0.9303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigramTagger/UnigramTagger</th>\n",
       "      <td>0.8830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrigramTagger/BigramTagger/UnigramTagger</th>\n",
       "      <td>0.8821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrigramTagger/UnigramTagger</th>\n",
       "      <td>0.8818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UnigramTagger</th>\n",
       "      <td>0.8773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UnigramTagger/BigramTagger</th>\n",
       "      <td>0.6997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigramTagger</th>\n",
       "      <td>0.6963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrigramTagger/BigramTagger</th>\n",
       "      <td>0.6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CountVectorizer_word</th>\n",
       "      <td>0.6703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfidfVectorizer_word</th>\n",
       "      <td>0.6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UnigramTagger/TrigramTagger</th>\n",
       "      <td>0.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HashingVectorizer_word</th>\n",
       "      <td>0.2883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigramTagger/TrigramTagger</th>\n",
       "      <td>0.2780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrigramTagger</th>\n",
       "      <td>0.2481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          accuracy\n",
       "CountVectorizer_char                        0.9500\n",
       "TfidfVectorizer_char                        0.9382\n",
       "HashingVectorizer_char                      0.9303\n",
       "BigramTagger/UnigramTagger                  0.8830\n",
       "TrigramTagger/BigramTagger/UnigramTagger    0.8821\n",
       "TrigramTagger/UnigramTagger                 0.8818\n",
       "UnigramTagger                               0.8773\n",
       "UnigramTagger/BigramTagger                  0.6997\n",
       "BigramTagger                                0.6963\n",
       "TrigramTagger/BigramTagger                  0.6892\n",
       "CountVectorizer_word                        0.6703\n",
       "TfidfVectorizer_word                        0.6692\n",
       "UnigramTagger/TrigramTagger                 0.3352\n",
       "HashingVectorizer_word                      0.2883\n",
       "BigramTagger/TrigramTagger                  0.2780\n",
       "TrigramTagger                               0.2481"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results).transpose(\n",
    ").sort_values(by='accuracy', ascending=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "* Среди одиночных теггеров - лучшие результаты показал UnigramTagger. Использование backoff позволило (незначительно) повысить точность\n",
    "* Применение предсказательных моделей на основе n-gram(CHAR) vectorizer-ов значительно повысило точность. Использование параметра analyzer='word' снижает точность предсказаний"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
