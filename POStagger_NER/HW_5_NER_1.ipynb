{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmJ3rYIgLGR1"
   },
   "source": [
    "## Задание 2. Проверить насколько хорошо работает NER\n",
    "данные брать из http://www.labinform.ru/pub/named_entities/\n",
    "1. взять нер из nltk\n",
    "2. написать свой нер попробовать разные подходы\n",
    "* передаём в сетку токен и его соседей\n",
    "* передаём в сетку только токен\n",
    "3. сделать выводы по вашим экспериментам какой из подходов успешнее справляется\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-ilb32PQLqhI"
   },
   "outputs": [],
   "source": [
    "from corus import load_ne5\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.ru.examples import sentences \n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RxcHSGU_MHPA"
   },
   "outputs": [],
   "source": [
    "# !wget http://www.labinform.ru/pub/named_entities/collection5.zip\n",
    "# !unzip collection5.zip\n",
    "# !rm collection5.zip\n",
    "\n",
    "# !pip install corus\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk2qSo5tMnb1",
    "outputId": "ec1e0f75-65c9-47b6-c542-3e3fa897736a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Собянин', 'JJ'),\n",
       " ('собрал', 'NNP'),\n",
       " ('необходимые', 'NNP'),\n",
       " ('подписи', 'NNP'),\n",
       " ('избирателей', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('свою', 'NNP'),\n",
       " ('поддержку', 'NNP'),\n",
       " ('ОНФ', 'NNP'),\n",
       " ('не', 'NNP')]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = 'Collection5/'\n",
    "records = load_ne5(dir)\n",
    "document = next(records).text\n",
    "nltk.pos_tag(nltk.word_tokenize(document))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-xrizCXOoBP",
    "outputId": "96baf3c5-a18e-4f4e-d584-5cd83e1677f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Собянин', 'PERSON'),\n",
       " ('Исаева', 'PERSON'),\n",
       " ('Сергей Железняк', 'PERSON'),\n",
       " ('Андрея Исаева', 'PERSON'),\n",
       " ('Сергей Нарышкин', 'PERSON'),\n",
       " ('Евгеньевич', 'ORGANIZATION'),\n",
       " ('ОНФ', 'ORGANIZATION'),\n",
       " ('Нарышкин', 'ORGANIZATION'),\n",
       " ('Госдумы', 'PERSON'),\n",
       " ('Галушка', 'PERSON')]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list({(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') })[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCNNdcMKSQqq"
   },
   "source": [
    "*****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3LVyZIyeU8h-"
   },
   "outputs": [],
   "source": [
    "# !pip  install razdel\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3MX0X310VG1C"
   },
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pfnuoXfVTVf",
    "outputId": "6367ff29-7dcc-43cd-b117-818db917e463"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         218941\n",
       "PER          21175\n",
       "ORG          13630\n",
       "LOC           4568\n",
       "GEOPOLIT      4356\n",
       "MEDIA         2480\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])\n",
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hSTiY-eiV5lB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XR59elmiqH1D"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m6ytZF5pqMNb"
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "train_data = train_data.batch(256)\n",
    "valid_data = valid_data.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kec8fA3oqZXw"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xppKYjHWqk_c"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    #ngrams=(1, 3),\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l-eRGt58q1q3"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.dr1 = Dropout(0.5)\n",
    "        self.fc2 = Dense(100, activation='relu')\n",
    "        self.dr2 = Dropout(0.25)\n",
    "        self.fc3 = Dense(50, activation='relu')\n",
    "        self.fc4 = Dense(6, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.dr1(fc_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        fc_x = self.dr2(fc_x)\n",
    "        fc_x = self.fc3(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc4(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSj9kpGir9Ot",
    "outputId": "ff739c57-e292-4194-a213-12a3e5bffaef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "777/777 [==============================] - 24s 29ms/step - loss: 0.4147 - accuracy: 0.8751 - val_loss: 0.2462 - val_accuracy: 0.9288\n",
      "Epoch 2/3\n",
      "777/777 [==============================] - 22s 28ms/step - loss: 0.1630 - accuracy: 0.9512 - val_loss: 0.2421 - val_accuracy: 0.9388\n",
      "Epoch 3/3\n",
      "777/777 [==============================] - 21s 27ms/step - loss: 0.1236 - accuracy: 0.9611 - val_loss: 0.2337 - val_accuracy: 0.9405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79f73f0fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x0DheUEhuW7N"
   },
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    ngrams=(1, 3),\n",
    "    output_sequence_length=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiQ7Pt0-uqI9",
    "outputId": "d13227e1-b903-4ece-fd1a-6539f7d7d441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "777/777 [==============================] - 22s 28ms/step - loss: 0.7244 - accuracy: 0.8233 - val_loss: 0.6951 - val_accuracy: 0.8259\n",
      "Epoch 2/3\n",
      "777/777 [==============================] - 21s 27ms/step - loss: 0.6976 - accuracy: 0.8257 - val_loss: 0.6934 - val_accuracy: 0.8259\n",
      "Epoch 3/3\n",
      "777/777 [==============================] - 21s 28ms/step - loss: 0.6952 - accuracy: 0.8257 - val_loss: 0.6932 - val_accuracy: 0.8259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79f7438fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel2 = modelNER()\n",
    "mmodel2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel2.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY-tQq1hz6jn"
   },
   "source": [
    "**Вывод:** Подача в нейронную сети ngram показало меньшую точность, по сравнению с подачей отдельных токенов"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"HW_5_NER.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
